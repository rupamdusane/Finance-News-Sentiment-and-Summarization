{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6e1f9a-c572-4a3b-8ac6-98610ac5abe7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yahoo_fin in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.9.1)\n",
      "Requirement already satisfied: feedparser in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.11)\n",
      "Requirement already satisfied: transformers in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.0+cu126)\n",
      "Requirement already satisfied: pandas in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.13.3)\n",
      "Collecting newspaper3k\n",
      "  Using cached newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests-html in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yahoo_fin) (0.10.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rupam\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from newspaper3k) (11.1.0)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from newspaper3k) (5.3.1)\n",
      "Collecting nltk>=3.2.1 (from newspaper3k)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
      "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "     ---------------------------------------- 0.0/7.4 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.3/7.4 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.5/7.4 MB 1.7 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 0.8/7.4 MB 1.5 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.0/7.4 MB 1.4 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 1.3/7.4 MB 1.5 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 1.8/7.4 MB 1.4 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 2.1/7.4 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 2.4/7.4 MB 1.4 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 2.6/7.4 MB 1.5 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 2.6/7.4 MB 1.5 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 3.1/7.4 MB 1.4 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 3.4/7.4 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 3.7/7.4 MB 1.4 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 4.2/7.4 MB 1.4 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 4.7/7.4 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 5.0/7.4 MB 1.5 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 5.5/7.4 MB 1.6 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 5.8/7.4 MB 1.6 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 6.3/7.4 MB 1.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 6.6/7.4 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.3/7.4 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.4/7.4 MB 1.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\rupam\\appdata\\roaming\\python\\python311\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
      "Collecting click (from nltk>=3.2.1->newspaper3k)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk>=3.2.1->newspaper3k)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
      "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pyquery in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html->yahoo_fin) (2.0.1)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html->yahoo_fin) (2.0.3)\n",
      "Requirement already satisfied: parse in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html->yahoo_fin) (1.20.2)\n",
      "Requirement already satisfied: bs4 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html->yahoo_fin) (0.0.2)\n",
      "Requirement already satisfied: w3lib in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html->yahoo_fin) (2.3.1)\n",
      "Requirement already satisfied: pyppeteer>=0.0.14 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests-html->yahoo_fin) (2.0.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (1.4.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (8.6.1)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (11.1.1)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (10.4)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html->yahoo_fin) (3.21.0)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
      "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13568 sha256=54124eda7686f7d13dbff87264816fb684914da54c7fece267394aaeeaecc8ce\n",
      "  Stored in directory: c:\\users\\rupam\\appdata\\local\\pip\\cache\\wheels\\fc\\ab\\f8\\cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=def2aeb0af128c2a66627f71e0f95fa18e12a6d61ecc74e75a76134b49298ca5\n",
      "  Stored in directory: c:\\users\\rupam\\appdata\\local\\pip\\cache\\wheels\\80\\d5\\72\\9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398386 sha256=f79091bdc47ec140b6b8fc0c1c575b6f57ef9a112a31560f97222e6cf43e9ae3\n",
      "  Stored in directory: c:\\users\\rupam\\appdata\\local\\pip\\cache\\wheels\\3a\\a1\\46\\8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k\n",
      "Installing collected packages: tinysegmenter, jieba3k, joblib, click, requests-file, nltk, feedfinder2, tldextract, newspaper3k\n",
      "Successfully installed click-8.1.8 feedfinder2-0.0.4 jieba3k-0.35.1 joblib-1.4.2 newspaper3k-0.2.8 nltk-3.9.1 requests-file-2.1.0 tinysegmenter-0.3 tldextract-5.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install yahoo_fin feedparser transformers torch pandas requests beautifulsoup4 newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68cff953-1064-49fc-bca5-f0b94b972ea9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml_html_clean\n",
      "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\rupam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lxml_html_clean) (5.3.1)\n",
      "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: lxml_html_clean\n",
      "Successfully installed lxml_html_clean-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml_html_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3c7479-3210-408b-90eb-7674825103f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import requests\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from newspaper import Article\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307ff9c1-3f43-4645-87d0-db5067687c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02dc9f61-280b-4b27-b55e-982970d8607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT model for financial sentiment analysis\n",
    "MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b43d81-35b2-4f58-a780-6d5e68365c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment using FinBERT\n",
    "def get_finbert_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cpu\")\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
    "    predicted_index = torch.argmax(predictions).item()\n",
    "    return sentiment_labels[predicted_index] if predicted_index < len(sentiment_labels) else \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcadb274-85fa-408a-984b-b792e6bd9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single article\n",
    "def process_article(entry, ticker):\n",
    "    url = entry.link\n",
    "    title = entry.title\n",
    "    publication_date = entry.published if 'published' in entry else 'N/A'\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "        \n",
    "        words = text.split(' ')[:1000]\n",
    "        ARTICLE = ' '.join(words) if len(words) >= 50 else text\n",
    "\n",
    "        if ARTICLE:\n",
    "            summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)\n",
    "            summary = summarizer(ARTICLE, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "\n",
    "            final_sentiment = get_finbert_sentiment(ARTICLE)\n",
    "            return [ticker, title, publication_date, summary, final_sentiment, url]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing article {url}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac45843b-dc7e-418e-a15d-e2b11f3ec881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 13:21:43,640 - ERROR - Error processing article http://www.etf.com/sections/news/stock-etfs-waver-investors-weigh-cpi-data-tariffs?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.etf.com/sections/news/stock-etfs-waver-investors-weigh-cpi-data-tariffs?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss on URL http://www.etf.com/sections/news/stock-etfs-waver-investors-weigh-cpi-data-tariffs?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 65. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "2025-03-13 13:22:29,917 - ERROR - Error processing article http://www.etf.com/sections/news/morningstar-european-investors-shifting-away-us-etfs?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.etf.com/sections/news/morningstar-european-investors-shifting-away-us-etfs?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss on URL http://www.etf.com/sections/news/morningstar-european-investors-shifting-away-us-etfs?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss\n",
      "2025-03-13 13:22:30,013 - ERROR - Error processing article http://www.etf.com/sections/daily-etf-flows/tlt-attracts-1-billion-investors-seek-safety?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.etf.com/sections/daily-etf-flows/tlt-attracts-1-billion-investors-seek-safety?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss on URL http://www.etf.com/sections/daily-etf-flows/tlt-attracts-1-billion-investors-seek-safety?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss\n",
      "2025-03-13 13:22:30,142 - ERROR - Error processing article http://www.etf.com/sections/news/morgan-stanley-sees-opportunity-european-etf-market?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.etf.com/sections/news/morgan-stanley-sees-opportunity-european-etf-market?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss on URL http://www.etf.com/sections/news/morgan-stanley-sees-opportunity-european-etf-market?utm_source=yahoo-finance&utm_medium=rss&utm_campaign=yahoo-finance-rss&.tsrc=rss\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 26. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Device set to use cpu\n",
      "2025-03-13 13:25:30,116 - ERROR - Error processing article https://www.fool.com/investing/2025/03/12/the-nasdaq-sell-off-has-made-these-3-great-growth/?source=eptyholnk0000202&utm_source=yahoo-host-full&utm_medium=feed&utm_campaign=article&referring_guid=0c855abe-e00b-4c2f-88e9-78ee88cac05c&.tsrc=rss: index out of range in self\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
      "2025-03-13 13:26:17,158 - ERROR - Error processing article https://www.thestreet.com/technology/elon-musk-gets-more-bad-news-as-big-tech-ceo-takes-over-his-rival?.tsrc=rss: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.thestreet.com/technology/elon-musk-gets-more-bad-news-as-big-tech-ceo-takes-over-his-rival?.tsrc=rss on URL https://www.thestreet.com/technology/elon-musk-gets-more-bad-news-as-big-tech-ceo-takes-over-his-rival?.tsrc=rss\n",
      "2025-03-13 13:26:17,221 - ERROR - Error processing article https://www.thestreet.com/automotive/analysts-rewire-tesla-stock-price-targets-on-delivery-forecasts-political-controversy?.tsrc=rss: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.thestreet.com/automotive/analysts-rewire-tesla-stock-price-targets-on-delivery-forecasts-political-controversy?.tsrc=rss on URL https://www.thestreet.com/automotive/analysts-rewire-tesla-stock-price-targets-on-delivery-forecasts-political-controversy?.tsrc=rss\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 115. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 75. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "2025-03-13 13:27:37,335 - ERROR - Error processing article https://finance.yahoo.com/news/ive-betrayed-tesla-drivers-pushing-203356733.html?.tsrc=rss: index out of range in self\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 59. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=29)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 101. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "2025-03-13 13:28:52,465 - ERROR - Error processing article https://uk.finance.yahoo.com/news/best-credit-card-deals-103741079.html?.tsrc=rss: index out of range in self\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "2025-03-13 13:29:58,706 - ERROR - Error processing article https://uk.finance.yahoo.com/news/best-credit-card-deals-103741079.html?.tsrc=rss: index out of range in self\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "2025-03-13 13:30:37,008 - ERROR - Error processing article https://uk.finance.yahoo.com/news/popular-stocks-investors-february-060031276.html?.tsrc=rss: index out of range in self\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
      "2025-03-13 13:31:04,681 - ERROR - Error processing article https://www.thestreet.com/retail/amazons-temu-shein-rival-has-an-unexpected-problem?.tsrc=rss: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.thestreet.com/retail/amazons-temu-shein-rival-has-an-unexpected-problem?.tsrc=rss on URL https://www.thestreet.com/retail/amazons-temu-shein-rival-has-an-unexpected-problem?.tsrc=rss\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 13. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=6)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 107. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 128. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=64)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "2025-03-13 13:32:18,670 - ERROR - Error processing article https://www.theinformation.com/briefings/ftc-continues-microsoft-antitrust-probe-trump?rc=66mtsr&.tsrc=rss: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.theinformation.com/briefings/ftc-continues-microsoft-antitrust-probe-trump?rc=66mtsr&.tsrc=rss on URL https://www.theinformation.com/briefings/ftc-continues-microsoft-antitrust-probe-trump?rc=66mtsr&.tsrc=rss\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n",
      "Device set to use cpu\n",
      "Your max_length is set to 150, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Device set to use cpu\n",
      "2025-03-13 13:33:56,627 - INFO - CSV file created: financial_news_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch articles using RSS feed\n",
    "def fetch_articles(ticker):\n",
    "    rss_feed = feedparser.parse(f'https://feeds.finance.yahoo.com/rss/2.0/headline?s={ticker}&region=US&lang=en-US')\n",
    "    return [(entry, ticker) for entry in rss_feed.entries[:10]] if rss_feed.entries else []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ARTICLE_LIST = []\n",
    "    unique_urls = set()\n",
    "\n",
    "    # Default tickers\n",
    "    TICKERS = ['SPY', 'AAPL', 'GOOGL', 'TSLA', 'BTC-USD', 'ETH-USD', 'AMZN', 'MSFT', 'DJI', 'IXIC', 'CRYPTO', 'FOREX']\n",
    "\n",
    "    article_entries = sum(map(fetch_articles, TICKERS), [])\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = list(executor.map(lambda args: process_article(*args), article_entries))\n",
    "    \n",
    "    for result in results:\n",
    "        if result and result[5] not in unique_urls:\n",
    "            ARTICLE_LIST.append(result)\n",
    "            unique_urls.add(result[5])\n",
    "\n",
    "    # Export to CSV\n",
    "    if ARTICLE_LIST:\n",
    "        output_file = 'financial_news_summary.csv'\n",
    "        df = pd.DataFrame(ARTICLE_LIST, columns=['Ticker', 'Title', 'Publication Date', 'Summary', 'Sentiment', 'URL'])\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"CSV file created: {output_file}\")\n",
    "    else:\n",
    "        logging.info(\"No articles fetched.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
